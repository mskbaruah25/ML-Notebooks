{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>In this notebook a custom implementation of logisitc regression using Stochastic Gradient Descent is Implemented and asserted</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>First we create a dataset and check the results for sklearn's implementation of Logisitc Regression</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gONY1YiDq7jD"
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(X_train)\n",
    "x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.76, NNZs: 15, Bias: -0.316130, T: 37500, Avg. loss: 0.456770\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.476828, T: 75000, Avg. loss: 0.395862\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.97, NNZs: 15, Bias: -0.588118, T: 112500, Avg. loss: 0.386856\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.01, NNZs: 15, Bias: -0.667250, T: 150000, Avg. loss: 0.383443\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.03, NNZs: 15, Bias: -0.726549, T: 187500, Avg. loss: 0.381825\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.769133, T: 225000, Avg. loss: 0.380862\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.801827, T: 262500, Avg. loss: 0.380467\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.827575, T: 300000, Avg. loss: 0.380167\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.846799, T: 337500, Avg. loss: 0.380044\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.860313, T: 375000, Avg. loss: 0.379963\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 10 epochs took 0.05 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42197358,  0.17755085, -0.14802932,  0.33585513, -0.20109342,\n",
       "          0.56054998, -0.43735215, -0.08422888,  0.22158259,  0.18265604,\n",
       "          0.18154486,  0.00304848, -0.07268168,  0.32830753,  0.02834674]]),\n",
       " (1, 15),\n",
       " array([-0.86031346]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we start implementing our custom made Logistic Regression using SGD</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "def initialize_weights(dim):\n",
    "    w = np.zeros_like(dim)\n",
    "    b = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    return w,b\n",
    "\n",
    "w,b =initialize_weights(X_train[0])\n",
    "print(np.shape(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(z):\n",
    "    sig = 1/(1+ math.exp(-z))\n",
    "    \n",
    "\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    loss1 = 0\n",
    "    for i in range(0, len(y_true)):\n",
    "        loss_instance = (y_true[i]* math.log10(y_pred[i])+ (1-y_true[i])*math.log10(1 - y_pred[i]))\n",
    "        loss1 = loss1 + loss_instance\n",
    "    loss = (-1*loss1)/len(y_true)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "   \n",
    "    \n",
    "    dw1 = np.dot(w, x.T) + b\n",
    "    dw2 = y - sigmoid(dw1)\n",
    "    dw3 = np.dot(x,dw2)\n",
    "    dw = dw3 - np.dot(alpha/N, w)\n",
    "    \n",
    "    return dw\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    "def gradient_db(x,y,w,b): \n",
    "        \n",
    "        db1 = np.dot(w,x.T) + b\n",
    "        db2 = sigmoid(db1)\n",
    "        db = y - db2 \n",
    "        \n",
    "        return db\n",
    "    \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  assert(grad_db==-0.5)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,-2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,3.67152472,  0.01451875,  2.01062888,\n",
    "0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    w,b = initialize_weights(X_train[0])\n",
    "    epoch_number = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for k in range(0,epochs):\n",
    "        \n",
    "        epoch_number.append(k)\n",
    "        for i in range(0,N):\n",
    "            dw = gradient_dw(X_train[i], y_train[i], w, b, alpha, len(X_train))\n",
    "            db = gradient_db(X_train[i], y_train[i], w, b)\n",
    "            w = w + (eta0*dw)\n",
    "            b = b + (eta0*db)\n",
    "        y_pred_train = [0 for i in range(N)]\n",
    "        for i in range(0,N):\n",
    "            y_pred_train[i] = sigmoid(np.dot(w,X_train[i].T)+b)  \n",
    "            \n",
    "        loss_train = logloss(y_train,y_pred_train)\n",
    "        train_loss.append(loss_train)\n",
    "        print(\"Epoch number \",k)\n",
    "        print(\"Loss train\",loss_train)\n",
    "        y_pred_test = [0 for i in range(len(y_test))]\n",
    "        for i in range(0,len(X_test)):\n",
    "            y_pred_test[i] = sigmoid(np.dot(w,X_test[i].T)+b)\n",
    "        loss_test = logloss(y_test,y_pred_test)\n",
    "        test_loss.append(loss_test)\n",
    "        print(\"Loss Test\",loss_test)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "    return w,b,epoch_number, train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number  0\n",
      "Loss train 0.17603563250170023\n",
      "Loss Test 0.17483126548515954\n",
      "====================================================================================================\n",
      "Epoch number  1\n",
      "Loss train 0.169253466920771\n",
      "Loss Test 0.1678232773887034\n",
      "====================================================================================================\n",
      "Epoch number  2\n",
      "Loss train 0.16697029331536084\n",
      "Loss Test 0.1655028547849492\n",
      "====================================================================================================\n",
      "Epoch number  3\n",
      "Loss train 0.16594160983629203\n",
      "Loss Test 0.16448570694661077\n",
      "====================================================================================================\n",
      "Epoch number  4\n",
      "Loss train 0.16542612294311743\n",
      "Loss Test 0.16399333059778456\n",
      "====================================================================================================\n",
      "Epoch number  5\n",
      "Loss train 0.1651543834332655\n",
      "Loss Test 0.16374490289232158\n",
      "====================================================================================================\n",
      "Epoch number  6\n",
      "Loss train 0.16500711021794334\n",
      "Loss Test 0.16361775792273955\n",
      "====================================================================================================\n",
      "Epoch number  7\n",
      "Loss train 0.1649259366193317\n",
      "Loss Test 0.1635528930120513\n",
      "====================================================================================================\n",
      "Epoch number  8\n",
      "Loss train 0.16488068851940565\n",
      "Loss Test 0.16352044372323646\n",
      "====================================================================================================\n",
      "Epoch number  9\n",
      "Loss train 0.16485525801831058\n",
      "Loss Test 0.1635048819310348\n",
      "====================================================================================================\n",
      "Epoch number  10\n",
      "Loss train 0.1648408722228886\n",
      "Loss Test 0.16349802851408593\n",
      "====================================================================================================\n",
      "Epoch number  11\n",
      "Loss train 0.16483268875262544\n",
      "Loss Test 0.16349556020168513\n",
      "====================================================================================================\n",
      "Epoch number  12\n",
      "Loss train 0.16482800923445634\n",
      "Loss Test 0.16349520230275613\n",
      "====================================================================================================\n",
      "Epoch number  13\n",
      "Loss train 0.16482531928572094\n",
      "Loss Test 0.16349577413214855\n",
      "====================================================================================================\n",
      "Epoch number  14\n",
      "Loss train 0.16482376422810643\n",
      "Loss Test 0.16349667864031905\n",
      "====================================================================================================\n",
      "Epoch number  15\n",
      "Loss train 0.16482285945254108\n",
      "Loss Test 0.16349762783373895\n",
      "====================================================================================================\n",
      "Epoch number  16\n",
      "Loss train 0.1648223290485833\n",
      "Loss Test 0.16349849485327536\n",
      "====================================================================================================\n",
      "Epoch number  17\n",
      "Loss train 0.16482201531275742\n",
      "Loss Test 0.1634992345639163\n",
      "====================================================================================================\n",
      "Epoch number  18\n",
      "Loss train 0.16482182774556348\n",
      "Loss Test 0.16349984131262219\n",
      "====================================================================================================\n",
      "Epoch number  19\n",
      "Loss train 0.16482171418683506\n",
      "Loss Test 0.16350032683258564\n",
      "====================================================================================================\n",
      "Epoch number  20\n",
      "Loss train 0.1648216444233773\n",
      "Loss Test 0.16350070901236813\n",
      "====================================================================================================\n",
      "Epoch number  21\n",
      "Loss train 0.16482160085074746\n",
      "Loss Test 0.16350100646411275\n",
      "====================================================================================================\n",
      "Epoch number  22\n",
      "Loss train 0.16482157313783125\n",
      "Loss Test 0.16350123613254966\n",
      "====================================================================================================\n",
      "Epoch number  23\n",
      "Loss train 0.1648215551691603\n",
      "Loss Test 0.1635014124520796\n",
      "====================================================================================================\n",
      "Epoch number  24\n",
      "Loss train 0.16482154328675416\n",
      "Loss Test 0.16350154725317553\n",
      "====================================================================================================\n",
      "Epoch number  25\n",
      "Loss train 0.16482153527526813\n",
      "Loss Test 0.16350164999850103\n",
      "====================================================================================================\n",
      "Epoch number  26\n",
      "Loss train 0.1648215297735051\n",
      "Loss Test 0.1635017281348398\n",
      "====================================================================================================\n",
      "Epoch number  27\n",
      "Loss train 0.16482152593126628\n",
      "Loss Test 0.16350178745723035\n",
      "====================================================================================================\n",
      "Epoch number  28\n",
      "Loss train 0.16482152320783122\n",
      "Loss Test 0.16350183243977018\n",
      "====================================================================================================\n",
      "Epoch number  29\n",
      "Loss train 0.16482152125265243\n",
      "Loss Test 0.16350186651712037\n",
      "====================================================================================================\n",
      "Epoch number  30\n",
      "Loss train 0.16482151983394752\n",
      "Loss Test 0.1635018923150949\n",
      "====================================================================================================\n",
      "Epoch number  31\n",
      "Loss train 0.1648215187954728\n",
      "Loss Test 0.16350191183505522\n",
      "====================================================================================================\n",
      "Epoch number  32\n",
      "Loss train 0.16482151802995415\n",
      "Loss Test 0.16350192659899565\n",
      "====================================================================================================\n",
      "Epoch number  33\n",
      "Loss train 0.16482151746248017\n",
      "Loss Test 0.16350193776242927\n",
      "====================================================================================================\n",
      "Epoch number  34\n",
      "Loss train 0.16482151703996753\n",
      "Loss Test 0.16350194620155314\n",
      "====================================================================================================\n",
      "Epoch number  35\n",
      "Loss train 0.16482151672431103\n",
      "Loss Test 0.16350195258013986\n",
      "====================================================================================================\n",
      "Epoch number  36\n",
      "Loss train 0.16482151648786106\n",
      "Loss Test 0.16350195740069576\n",
      "====================================================================================================\n",
      "Epoch number  37\n",
      "Loss train 0.16482151631038622\n",
      "Loss Test 0.16350196104344253\n",
      "====================================================================================================\n",
      "Epoch number  38\n",
      "Loss train 0.16482151617696725\n",
      "Loss Test 0.16350196379595772\n",
      "====================================================================================================\n",
      "Epoch number  39\n",
      "Loss train 0.16482151607655054\n",
      "Loss Test 0.16350196587568958\n",
      "====================================================================================================\n",
      "Epoch number  40\n",
      "Loss train 0.16482151600090209\n",
      "Loss Test 0.163501967447019\n",
      "====================================================================================================\n",
      "Epoch number  41\n",
      "Loss train 0.16482151594387862\n",
      "Loss Test 0.1635019686341915\n",
      "====================================================================================================\n",
      "Epoch number  42\n",
      "Loss train 0.16482151590086994\n",
      "Loss Test 0.16350196953110616\n",
      "====================================================================================================\n",
      "Epoch number  43\n",
      "Loss train 0.16482151586841626\n",
      "Loss Test 0.1635019702087173\n",
      "====================================================================================================\n",
      "Epoch number  44\n",
      "Loss train 0.1648215158439225\n",
      "Loss Test 0.1635019707206384\n",
      "====================================================================================================\n",
      "Epoch number  45\n",
      "Loss train 0.16482151582543259\n",
      "Loss Test 0.1635019711073817\n",
      "====================================================================================================\n",
      "Epoch number  46\n",
      "Loss train 0.16482151581147125\n",
      "Loss Test 0.16350197139955397\n",
      "====================================================================================================\n",
      "Epoch number  47\n",
      "Loss train 0.16482151580092816\n",
      "Loss Test 0.16350197162028052\n",
      "====================================================================================================\n",
      "Epoch number  48\n",
      "Loss train 0.16482151579296608\n",
      "Loss Test 0.16350197178702966\n",
      "====================================================================================================\n",
      "Epoch number  49\n",
      "Loss train 0.16482151578695317\n",
      "Loss Test 0.16350197191300234\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b,epoch_number, train_loss, test_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in weight vector and intercept:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-1.14882430e-02, -4.75734194e-05, -1.01219114e-03,\n",
       "         -6.98063402e-03, -1.42415593e-02, -3.69510718e-04,\n",
       "         -3.88601156e-03,  2.81668369e-03, -3.19929042e-03,\n",
       "          2.38949614e-03,  8.00930605e-03, -6.08677645e-03,\n",
       "         -7.58179462e-03, -2.54515866e-03,  1.92687284e-03]]),\n",
       " array([-0.03850128]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Difference in weight vector and intercept:\")\n",
    "w-clf.coef_, b-clf.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>In the above we can see that the difference in coefficients from sklearn's algorithm and the custom implemented algorithm is very less and this should assert that the custom implementation is working properly</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_number,train_loss, label = \"Train loss\" )\n",
    "plt.plot(epoch_number,test_loss, label = \"Test Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"ERROR PLOTS\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy achieved through the custom implementation 0.9519466666666667\n",
      "Train accuracy achieved through the custom implementation 0.94584\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: \n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "print(\"Train accuracy achieved through the custom implementation\",1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(\"Train accuracy achieved through the custom implementation\",1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k28U1xDsLIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMokBfs3-2PY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
